{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indeed_Test(scrapy.Spider):\n",
    "    name = \"jack\"\n",
    "    def start_requests( self ):\n",
    "        url = 'https://www.indeed.com/jobs?q=Data%20Scientist&l=Boston,%20MA&start=40'\n",
    "        yield scrapy.Request( url = url,\n",
    "                              callback = self.parse_front )\n",
    "    def parse_front( self, response ):\n",
    "        # simple example: write out the html\n",
    "        match='h2.title'\n",
    "        links_block = response.css(match)\n",
    "        print(links_block.xpath('./a/@href').extract())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class page_download(scrapy.Spider):\n",
    "    name = \"jack123\"\n",
    "    def start_requests( self ):\n",
    "        url = 'https://www.indeed.com/jobs?q=Data+Science&l=Boston%2C+MA&start=10'\n",
    "        yield scrapy.Request( url = url,\n",
    "                              callback = self.parse_front )\n",
    "    def parse_front( self, response ):\n",
    "        match='h2.title'\n",
    "        links_block = response.css(match)\n",
    "        # Extract the links (as a list of strings)\n",
    "        links_to_follow = links_block.xpath('./a/@href').extract()\n",
    "        # Follow the links to the next parser\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow( url = \"https://www.indeed.com/\"+url,\n",
    "                                   callback = self.parse_pages )\n",
    "    def parse_pages(self,response):\n",
    "        time = time.time()\n",
    "        #file_name= str(\"D:/Courses/NEU/INT5964_XN/TestfilesFile\") + str(time) + str(\".html\")\n",
    "        file_name='test.html'\n",
    "        with open(file_name, 'wb') as fout:\n",
    "            fout.write(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class page_test(scrapy.Spider):\n",
    "    \n",
    "    name = \"jack\"\n",
    "    def start_requests( self ):\n",
    "        url = 'https://www.indeed.com/viewjob?jk=80f2b213b92bc68b&q=Data+Science&l=Boston,+MA'\n",
    "        yield scrapy.Request( url = url,\n",
    "                              callback = self.parse_front )\n",
    "    def parse_front( self, response ):\n",
    "        # simple example: write out the html\n",
    "        job_description_match='div.jobsearch-jobDescriptionText p ::text,div.jobsearch-jobDescriptionText li ::text, div.jobsearch-jobDescriptionText div ::text'\n",
    "        company_match='div.icl-u-lg-mr--sm.icl-u-xs-mr--xs::text'\n",
    "        ratings_match='div.icl-Ratings-count::text'\n",
    "        title_match='head > title::text'\n",
    "        name_match='h3.icl-u-xs-mb--xs.icl-u-xs-mt--none.jobsearch-JobInfoHeader-title::text'\n",
    "        location_match='div.icl-u-lg-mr--sm.icl-u-xs-mr--xs > div::text'\n",
    "        #cl-u-lg-mr--sm icl-u-xs-mr--xs\n",
    "        job_description=response.css(job_description_match).extract()\n",
    "        company_name=response.css(company_match).extract_first()\n",
    "        title=response.css(title_match).extract()\n",
    "        ratings=response.css(ratings_match).extract_first()\n",
    "        name=response.css(name_match).extract()\n",
    "        #location=response.css(location_match).extract()\n",
    "        print(str(\"ratings:\") + str(ratings))\n",
    "        print(str(\"title:\") + str(title))\n",
    "        print(str(\"company_name:\") + str(company_name))\n",
    "        print(str(\"name\")+ str(name))\n",
    "        #print(str(\"location:\") + str(location))\n",
    "        dc['title'] =title\n",
    "        dc['company_name'] = company_name\n",
    "        dc['name'] = name\n",
    "        dc['ratings'] = ratings\n",
    "        dc['job_description'] = job_description\n",
    "        df= df.append({'title':title, 'company_name':company_name, 'name':name, 'ratings':ratings, 'job_description':job_description}, ignore_index=True)\n",
    "        #df.append(dc)\n",
    "        \n",
    "        #file_name='test.html'\n",
    "        #with open(file_name, 'wb') as fout:\n",
    "        #    fout.write(response.body)\n",
    "        #time = time.time()\n",
    "        #file_name1= 'D:/Courses/NEU/INT5964_XN/TestfilesFile.html'\n",
    "        #file_name='test.html'\n",
    "        #with open(file_name1, 'wb') as fout:\n",
    "        #    fout.write(response.body)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-08 10:09:07 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: scrapybot)\n",
      "2020-06-08 10:09:07 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0\n",
      "2020-06-08 10:09:07 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2020-06-08 10:09:07 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2020-06-08 10:09:07 [scrapy.extensions.telnet] INFO: Telnet Password: 95c2aff07047a8ff\n",
      "2020-06-08 10:09:08 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-06-08 10:09:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-06-08 10:09:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-06-08 10:09:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-06-08 10:09:09 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-06-08 10:09:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-06-08 10:09:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-06-08 10:09:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.indeed.com/viewjob?jk=80f2b213b92bc68b&q=Data+Science&l=Boston,+MA> (referer: None)\n",
      "2020-06-08 10:09:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.indeed.com/viewjob?jk=80f2b213b92bc68b&q=Data+Science&l=Boston,+MA> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\JG\\anaconda3\\lib\\site-packages\\twisted\\internet\\defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-4-f07b549e4866>\", line 33, in parse_front\n",
      "    df= df.append({'title':title, 'company_name':company_name, 'name':name, 'ratings':ratings, 'job_description':job_description}, ignore_index=True)\n",
      "UnboundLocalError: local variable 'df' referenced before assignment\n",
      "2020-06-08 10:09:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-06-08 10:09:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 269,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 62583,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.642561,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 6, 8, 14, 9, 10, 88027),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/UnboundLocalError': 1,\n",
      " 'start_time': datetime.datetime(2020, 6, 8, 14, 9, 9, 445466)}\n",
      "2020-06-08 10:09:10 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings:80 reviews\n",
      "title:['Data Scientist Internship - Boston, MA 02111 - Indeed.com']\n",
      "company_name:-\n",
      "name['Data Scientist Internship']\n",
      "Empty DataFrame\n",
      "Columns: [title, company_name, name, ratings, job_description]\n",
      "Index: []\n",
      "{'title': ['Data Scientist Internship - Boston, MA 02111 - Indeed.com'], 'company_name': '-', 'name': ['Data Scientist Internship'], 'ratings': '80 reviews', 'job_description': ['Overview:', '\\nInvention and Innovation in the insurance industry relies heavily on predictive modeling. In this role, you will be set on a constant “treasure hunt” for new data sources to apply the latest techniques for modeling to. You will have unparalleled opportunity to apply state-of-the-art machine learning techniques to real world datasets. You will work with highly intelligent and accomplished individuals who understand and value technical challenges. You will work on a team of PhD level statisticians to create ground-breaking predictive models using new analytical techniques that determine insurance pure premiums, customer life time value, customer buying behaviors and other related topics. This work will be used in setting pricing, underwriting, marketing, policy and claims service, fraud detection and product design strategies for the Plymouth Rock group of Companies. The role is not simply research: you will be part of a tactical team driving the profit and customer growth of an assigned business segment. The analysis and modeling will be tested via implementation both broadly and on a pilot basis to allow us to learn and refine our approaches in rapid succession.', '\\n', '\\nData Analysis:\\n', 'You will work with internal and external data to understand and accurately predict:', '\\nExpected insurance losses;', '\\nShopping behavior;', '\\nRetention behavior;', '\\nMost compelling value propositions;\\n', 'Awareness and consideration of brand;', '\\nQuote and conversion rates;\\n', 'Retention and lifetime customer value', '\\nContribution to overall margin;', '\\nAppropriate and actuarially sound risk pooling;', '\\nLoss cost and claim frequency trends / inflation;', '\\nCustomer satisfaction;', '\\nRouting and service step-by-step design;', '\\nFraud detection and handling strategy;', '\\n“Unpriced risk” existence and tracking; and', '\\nOther drivers of success you identify worthy of analysis.', '\\nYou will need to find and understand various internal and external data sources as well as obtain and cleanse such data to make it usable and to extract, clean, and manipulate large datasets (structured and unstructured) for model building. You will develop illicit hypothesis from internal personnel regarding key variables and test those hypotheses using available data. You will design, build and append to a data warehouse and DataMart connecting data with common categorical and identifying descriptors, and make the data and your derived analytical measures to production systems, underwriters and other personnel.', '\\n', '\\nModeling:', '\\nYou will develop and enhance models used in a variety of contexts within the product management structure of Plymouth Rock by utilizing both statistical methods (Generalized Linear Models, Mixed Effects Models, Ridge, Lasso, etc.) and machine learning techniques (Gradient Boosted Trees, Random Forests, Deep Learning, etc.).. You will stay current on the latest machine learning and big data trends and be responsible for advancing the modeling capabilities within Plymouth Rock and transferring those advancements to other personnel.', '\\n', '\\nImplementation:', '\\nYou will facilitate the application of models developed considering the regulatory, distribution and competitive contexts within which we operate. This will include ensuring that models are well understood, the implications are appropriately assessed, implementation plans are defined, validation of results are undertaken and adjustments are made rapidly. You will work with business sponsors and IT teams to implement analytic solutions.', '\\n', '\\nThe successful candidate will have the following attributes:', '\\nAn advanced degree in mathematics, machine learning, economics, statistics or a related field.', '\\nKnowledge of new and innovative modeling and statistical techniques – a relevant specialty is preferred.', '\\nDetail orientation – attention and desire to work on large data “crunching” projects.', '\\nStrong written and oral communication skills and a proven ability to communicate technical concepts to both technical and non-technical audiences.', '\\nHigh standards for the work products delivered.', '\\nCreative thinker with the ability to synthesize information from various sources and apply that information to concrete business problems.', '\\nAbility to influence and guide across departmental boundaries.', '\\nFlexibility and resiliency characteristic of a professional.', '\\nAbility to work with and understanding of the merits of various data structures.', '\\nProficiency with the latest modeling software and techniques, such as SAS, R, Python or Tableau.', '\\nA desire - enthusiasm - to “change the game.”，passionate about tackling sample bias, over-fitting, variable selection, missing values, etc.', '\\nAbility to understand the need to balance predictive power, interpretability, and ease of implementation.']}\n"
     ]
    }
   ],
   "source": [
    "dc=dict()\n",
    "df = pd.DataFrame(columns=[\"title\",\"company_name\",\"name\",\"ratings\", \"job_description\"])\n",
    "first_page = CrawlerProcess()\n",
    "first_page.crawl(page_test)\n",
    "first_page.start()\n",
    "print(df)\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
